---
title: "Blog Three"
author: "Jackson Delgado"
date: '2022-09-26'
output: md_document
---

# Blog Post Three: Polls

### September 19, 2022

[Back to Homepage](../../README.md)

[View the Code](https://github.com/jrdelgado2018/GOV1347/blob/master/blogs/blog3/Blog%20Three.Rmd)

```{r setup, include=FALSE}
# Set R Markdown settings and load necessary packages
knitr::opts_chunk$set(echo=FALSE, fig.align="center")
knitr::opts_knit$set(root.dir="~/Desktop/GOV1347.nosync")
library(tidyverse)
library(gridExtra)
library(usmap)
```

## The Economist's and FiveThirtyEight's Approaches

In this blog post, I will summarize the modeling approaches taken by two major political forecasters, The Economist and FiveThirtyEight. I will compare and contrast the two approaches, ultimately expressing a slight preference for FiveThirtyEight's methodology, and will then update my predictive model using a couple of insights from both groups. 

### The Economist

In a nutshell, The Economist approaches their prediction task in a two step process. The first step is to estimate the overarching national political sentiment - the nationwide popular vote shares that will be achieved by Democrats and Republicans. They use polling to inform this estimate. The primary source is the generic ballot poll, but they also consider polls of the president's popularity (theorizing that a very popular president will tend to help their party's congressional candidates, and vice versa). They adjust the estimates provided by the polls with such factors as the presence of uncontested seats, the degree of political polarization, and "fundamental" factors like the state of the economy and whether the election is a midterm. 

In the second stage, The Economist uses this nationwide vote share forecast to individually predict each district. The idea is to use historical election data in that district to see how it tends to deviate from the national baseline. In addition to the raw historical vote shares, The Economist considers the presence of swing voters in the district, and structural factors like whether an incumbent is running, how much they've raised, and where they fall ideologically. Instead of making a point estimate from these factors, The Economist fits a skew-T distribution for each district, attempting to capture not just the most likely outcome but also the extreme events that might occur. With these distributions fitted, they make their final seat forecasts based on the proportion of seats won for each party in 10,000 simulations of the election.

### FiveThirtyEight

This model predicts the midterm election in one step. The primary tool for this prediction is polling - and FiveThirtyEight uses pretty much any poll that they can get their hands on, as long as it comes from a professional source. Reflecting Galton's observation that an aggregation of many guesses will tend to outperform one single guess, FiveThirtyEight takes great care to weight each poll appropriately. Weighting factors include how recently the poll was taken, the demographic of the poll (registered voters vs. likely voters), and the track record of the specific pollster. However, every single district will not have a poll dedicated to it, since only the most contested/followed/"interesting" ones will. So, FiveThirtyEight employs a k-nearest neighbors algorithm to predict what a hypothetical poll would say about an unpolled district, keeping in mind the selection bias of polls only being conducted in the more salient districts. 

In addition to the polls, this model includes a diverse variety of fundamental variables that are known to impact the outcomes of elections. FiveThirtyEight includes on this list such factors as incumbency status, the margin of victory for the incumbent in the previous election, the generic ballot poll (as a measure of the national sentiment), campaign funds raised, the performance of the district in previous presidential and state legislative elections, and the political experience of the challenger. In races for open seats, only the relevant factors (so, those that do not relate to an incumbent) are included. One version of the FiveThirtyEight predictor even uses the ratings ("toss up", "leans Democrat", etc.) that historically accurate political experts have assigned each district. With all this in mind, FiveThirtyEight simulates many runs of the election, taking special care to model the correlation structure of the districts, to make its final prediction. 

### Comparing the Methods

The most obvious difference between the two methods is their overall structure. The Economist predicts a national trend and each district's subsequent deviations from that trend, while FiveThirtyEight focuses exclusively on the idiosyncracies of each district. Within that structure, the models have many similarities in terms of the included variables. Both models include the fundamental factors that reflect the effect of structure on congressional elections - incumbency status, fundraising, whether it's a midterm, and so on. The one notable difference in fundamentals is that The Economist includes the state of the economy (as indicated by the unemployment rate), while FiveThirtyEight does not include this (though, one might argue that it's inclusion of Congress's approval rating is relatively correlated to the state of the economy).

In terms of included variables, the major difference between the models lies not in the fundamentals but in the poll data. The Economist relies mostly on the generic ballot poll, while FiveThirtyEight uses a much wider variety of more specific polls (including the generic ballot poll in the fundamentals category as a representation of national sentiment). The reason for this difference is the reality that most individual districts will not have a poll targeted to it; the two forecasters just solve this problem with different methods. FiveThirtyEight applies k-nearest neighbors prediction to carefully-weighted averages of nearby polls, whereas The Economist instead ignores district-level polling and uses polls to predict a national baseline from which districts deviate in predictable ways.

Both forecasters take care to avoid giving too much weight to a single point estimate, but they again go about achieving this goal in different ways. The Economist uses elastic-net regularization to avoid large model coefficients that overfit the data, and then estimate a skew-T distribution (notorious for flexible shape and fat tails) for each district in order to adequately predict extreme events (making no mention of correlation between districts). FiveThirtyEight, on the other hand, makes no mention of regularization to scale down fitted coefficients, but they take great care to model the correlation structure between the error of each district's estimate (taking into account four categories that describe how polls and fundamentals might over/understate certain factors across multiple districts). Both FiveThirtyEight and The Economist use simulation to reach their final predictions, most likely because the probability calculations required to find an analytic distribution would be intractable. 

Both models obviously include a lot of thought, and were crafted by experienced data scientists. However, I believe FiveThirtyEight's methodology to be slightly better, for two reasons. Firstly, while deviations from a national baseline is certainly a logical framework for this prediction task, it introduces two possible sources of error in the model: error in predicting the national vote share, and error in translating the national vote share to district vote shares. These errors might compound on each other in an "unlucky" realization of the election, so I marginally prefer FiveThirtyEight's single-stage model, with only one prediction step in which an error could be made. Secondly, The Economist provided no justification for their assumption that each district follows a skew-T distribution, and it is well-known to be more difficult to learn an entire distribution than to learn a single parameter. So, while a fat-tailed and skewed distribution certainly makes sense, I again have a marginal preference for FiveThirtyEight's approach of modeling the correlation structure between districts instead of the distributions of independent districts (especially considering the correlated polling errors we saw in 2016!). Again, though, I find both forecasts to be incredibly impressive. 

```{r data, include=FALSE}
# Read in the voting data, filter for elections where there was a Democrat and a Republican and third parties were < 10% of the vote, 
# calculate the statewide vote percent that Democrats won, and create a column for the sitting president being a Democrat
votes = read_csv("data/house party vote share by district 1948-2020.csv") %>% 
  filter(ThirdVotesTotalPercent < 10 & !is.na(RepCandidate) & !is.na(DemCandidate)) %>%
  group_by(raceYear, State) %>% 
  summarize(DemPct = 100 * sum(DemVotes) / (sum(DemVotes) + sum(RepVotes)), .groups = 'drop') %>%
  mutate(DemPresident = raceYear %in% c(1978, 1980, 1994, 1996, 1998, 2000, 2010, 2012, 2014, 2016))

# Create a dataframe with the relevant variables from the prior election (will need this later)
df_prev = votes %>%
  rename(PctPrev = DemPct) %>%
  mutate(raceYear = raceYear + 2) %>%
  select(raceYear, State, PctPrev)

# Read in the state unemployment data, filtering for the Q3 and Q4 numbers
unemp_Q4 = read_csv("data/unemployment_state_monthly.csv") %>% 
  filter(Month == 10) %>%
  rename(UnempQ4 = Unemployed_prct) %>%
  select(Year, `State and area`, UnempQ4)
unemp_Q3 = read_csv("data/unemployment_state_monthly.csv") %>% 
  filter(Month == 7) %>%
  rename(UnempQ3 = Unemployed_prct) %>%
  select(Year, `State and area`, UnempQ3)

# Join the unemployment data together
unemp = inner_join(unemp_Q4, unemp_Q3, c("Year", "State and area"))

# Get the poll data, filter for polls within 2 months of the election, and take simple average of results
polls = read_csv("data/polls_df.csv") %>% 
  filter(emonth %in% c(10, 11)) %>%
  mutate(isDemocrat = party == "D") %>%
  group_by(year) %>%
  summarize(D = sum(support * isDemocrat) / sum(isDemocrat), R = sum(support * (1 - isDemocrat)) / sum(1 - isDemocrat)) %>%
  mutate(DemPolls = D / (D + R))

# Join the data all together, calculating a couple of extra columns
df = votes %>% 
  inner_join(df_prev, c("raceYear", "State")) %>%
  inner_join(unemp, c("raceYear" = "Year", "State" = "State and area")) %>%
  inner_join(polls, c("raceYear" = "year")) %>%
  mutate(UnempPctChange = 100 * (UnempQ4 - UnempQ3) / UnempQ3,
         DemTimesUnemp = DemPresident * UnempQ4, 
         DemTimesPolls = DemPresident * DemPolls, 
         DemTimesPct = DemPresident * UnempPctChange)
```

## 2022 Forecast Update

In line with the FiveThirtyEight model, I update my model (that predicts vote share on a state-by-state level) to include the generic ballot poll. In accordance with the "wisdom of crowds" logic, I take a simple average of all generic ballot polls conducted within two months of the election - though, in the future, I would like to consider my weighting more carefully, as FiveThirtyEight does. For each poll average, I calculate the two-party vote share that would go to the Democrats. I also allow the presence of a Democrat in office to interact with the generic poll results, as voters might respond more favorably to Democrats if they are popular and a Republican president is in office. 

Interestingly, in this model, the economic variable that becomes more significant is the Q7-Q8 percent change in unemployment rate, not simply the Q8 unemployment rate. Last week, I concluded that voters tend to respond more to the absolute Q8 rate, so does this finding contradict that? It's not quite that simple. Last week's model isolated the effect of unemployment on voter choice, but this week's model includes another variable (national sentiment via generic ballot) in the equation. So, an interpretation could be that once national sentiment is accounted for, voters respond more to the change in unemployment (perhaps because national sentiment is responsive to the overall level of unemployment).

Or, perhaps this just happened due to random chance. The coefficient on unemployment is significant, but still low - on the same order as it was last week. So, it might be the case that in this new model, since the effect of unemployment is relatively low in magnitude, the inclusion of a new covariate caused the percent change to become more significant due to pure chance. Indeed, the overall model is not very much improved from last week - the R-squared jumped by just around 1%, but the bootstrapped estimate of the root-mean squared decreases only to 6.51. In the future, the model might be improved by calibrating each state's sensitivity to the national generic ballot poll separately (as, some states swing more than others), and by looking into district-level polls in addition to the generic ballot (as FiveThirtyEight does).

The two figures below include a scatterplot of the actual vote share to Democrats (each point is one state in one year) versus the predicted vote share. The fit looks pretty similar to last week's fit (for reference, the dashed line indicates where the actual value would equal the predicted). On the right is a histogram of the residuals, with a Normal density overlayed. Again, the residuals appear normal, justifying our statistical inferences about this model. 

```{r model2, include=FALSE}
# Model incorporating the previous vote share, the unemployment level, and generic ballot polling
model2 = lm(DemPct ~ PctPrev + DemPolls + DemTimesPolls + UnempPctChange + DemTimesPct, df)
df$Predicted2 = model2$fitted.values
summary(model2)

# Bootstrapped estimate of the MSE using cross-validation
mses = replicate(10000, {
  in_sample_inds = sample.int(n=nrow(df), size=floor(0.75*nrow(df)), replace=FALSE)
  in_sample = df[in_sample_inds, ]
  out_sample = df[-in_sample_inds, ]
  mod = lm(DemPct ~ PctPrev + DemPolls + DemTimesPolls + UnempPctChange + DemTimesPct, in_sample)
  preds = predict(mod, out_sample)
  mean((preds - out_sample$DemPct) ** 2)
})
mse = mean(mses)
rmse = sqrt(mse)
```

```{r model3, include=TRUE, fig.width=10, fig.height=5}
# Scatterplot of predicted vs. actual vote share
p3 = df %>% ggplot(aes(Predicted2, DemPct)) +
  geom_point() + 
  geom_abline(slope=1, intercept=0, lty=2) +
  xlab("Predicted Vote Share Achieved by Democrats") +
  ylab("Actual Vote Share Achieved by Democrats") +
  ggtitle("(vote share) = \n2.85 + 0.77 (previous vote share) + \n(0.10 if G.O.P. president else -0.12) (Q7-Q8 unemp. % change) + \n(18.2 if G.O.P. president else 13.2) (generic ballot avg.)")

# Histogram of the residual with overlayed Normal density curve
p4 = df %>% ggplot(aes(Predicted2 - DemPct)) + 
  geom_histogram(aes(y = ..density..), binwidth=4) + 
  xlab("Residual") + 
  ylab("Density") + 
  stat_function(fun = function(x) dnorm(x, 0, summary(model2)$sigma)) + 
  ggtitle("Histogram of Residuals")

grid.arrange(p3, p4, ncol=2)
```

Below is a map that summarizes the point estimates of this model's prediction of the 2022 election, using the most current available unemployment data and generic poll data. South Dakota again does not have a prediction because there is no Democrat running in the state. A color closer to blue corresponds to a larger vote share for Democrats, and a color closer to white corresponds to a smaller vote share for Democrats. Below is also a table that gives a 95% prediction interval for each state.

```{r map, include=TRUE, fig.width=10, fig.height=5, message=FALSE}
# Get the polls for today
polls_today = read_csv("data/538_generic_ballot_averages_2018-2022.csv") %>% 
  filter(month == 9 & cycle == 2022) %>%
  mutate(isDemocrat = candidate == "Democrats") %>%
  group_by(cycle) %>%
  summarize(D = sum(pct_estimate * isDemocrat) / sum(isDemocrat), R = sum(pct_estimate * (1 - isDemocrat)) / sum(1 - isDemocrat)) %>%
  mutate(DemPolls = D / (D + R))

# Read in the state unemployment data, filtering for the Q3 and Q4 numbers
unemp_Q4_today = read_csv("data/unemployment_state_monthly.csv") %>% 
  filter(Month == 5 & Year == 2022) %>%
  rename(UnempQ4 = Unemployed_prct) %>%
  select(Year, `State and area`, UnempQ4)
unemp_Q3_today = read_csv("data/unemployment_state_monthly.csv") %>% 
  filter(Month == 2 & Year == 2022) %>%
  rename(UnempQ3 = Unemployed_prct) %>%
  select(Year, `State and area`, UnempQ3)
unemp_today = inner_join(unemp_Q4_today, unemp_Q3_today, c("Year", "State and area"))

# Get the data specifically for today
today = unemp_today %>%
  inner_join(filter(df_prev, raceYear == 2022), c("State and area" = "State")) %>%
  rename(state = `State and area`) %>%
  inner_join(polls_today, c("Year" = "cycle")) %>%
  mutate(UnempPctChange = 100 * (UnempQ4 - UnempQ3) / UnempQ3,
         DemTimesPct = UnempPctChange, 
         DemTimesPolls = DemPolls)

# Get the predictions given by this model
today$Predicted = data.frame(predict(model2, today, interval="prediction"))$fit
today$LowerBound = data.frame(predict(model2, today, interval="prediction"))$lwr
today$UpperBound = data.frame(predict(model2, today, interval="prediction"))$upr

# Plot a US map with the predictions
states = us_map()
plot_usmap(data=today, regions="states", values="Predicted") +
  theme_void() + 
  scale_fill_gradient(low="white", high="blue", name="% of Votes", limits=range(30, 70)) + 
  ggtitle("Predicted Vote Share Achieved by Democrats, 2022 Election")

# Print the predictions and the prediction interval
print(data.frame(select(today, state, LowerBound, Predicted, UpperBound)), digits = 3)
```

```{r corr, include=FALSE}
corr_df = df %>% mutate(Diff = DemPct - Predicted2) %>% select(raceYear, State, Diff) %>% spread(State, Diff) %>% drop_na() %>% select(-raceYear)
cor()
```