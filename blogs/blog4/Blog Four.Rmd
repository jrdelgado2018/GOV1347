---
title: "Blog Four"
author: "Jackson Delgado"
date: '2022-10-03'
output: md_document
---

# Blog Post Four: Expert Predictions and Fundamentals

### October 3, 2022

[Back to Homepage](../../README.md)

[View the Code](https://github.com/jrdelgado2018/GOV1347/blob/master/blogs/blog4/Blog%20Four.Rmd)

```{r setup, include=FALSE}
# Set R Markdown settings and load necessary packages
knitr::opts_chunk$set(echo=FALSE, fig.align="center")
knitr::opts_knit$set(root.dir="~/Desktop/GOV1347.nosync")
library(tidyverse)
library(gridExtra)
library(usmap)
library(sf)
library(rmapshaper)
```

## Expert Ratings

In this blog post, I will compare the results of the 2018 midterm elections to the ratings that political experts assigned to each district. The political forecasters I examine are [The Cook Political Report](https://www.cookpolitical.com/ratings/house-race-ratings), [Sabato's Crystal Ball](https://centerforpolitics.org/crystalball/2022-house/), and [Inside Elections](https://insideelections.com/ratings/house), since these three made a forecast for all 435 congressional districts and the data is accessible on [Ballotpedia](https://ballotpedia.org/Main_Page). 

Each forecaster assigns each district a categorical rating from a set of seven ratings - the district can be "solid"/"safe", "likely", or "lean" towards one party, or the district can be a "toss up" between the two parties. These ratings had to be transformed to a numeric scale so that they could be compared to measurable election outcomes, so I assigned each rating a number from 1 to 7 (where 1 is "solid/safe Democrat", 7 is "solid/safe Republican", and 4 is "toss up"). 

The goal is to compare these expert predictions to the election outcome (I used Republican two-party vote share; there was negligible difference when Republican vote margin was used instead), but the two variables are currently on different scales. Vote share (when measured as a percent) takes on values from 0 to 100, whereas the expert ratings as I've defined them take on values from 1 to 7. To put these variables on the same scale, I calculated the percentile of each observation (which is the percent of other observations that are lower). I considered simply assigning a vote share to each expert categorical rating (perhaps a 4 corresponds to a prediction of 50% for Republicans, a 5 corresponds to a prediction of 52% for Republicans, and so on), but the problem is that the experts make their ratings in accordance with the (predicted) probability of the Republican winning the seat. And while correlated, win probabilities do not map nicely onto vote shares, so I did not want to implicitly assume that they do. So, I opted to use this normalization approach to transform both variables into distributions that range from 0 to 100.

The maps below compare the percentile of the Republican vote share (on the left) to the percentile of the average expert prediction (on the right) for each district. A redder district is one that was stronger for the Republicans, and a bluer district is one that was weaker for the Republicans. 

```{r expertdata, include=FALSE}
experts2018 = read_csv("data/2018_ratings_share.csv")

votes2018 = read_csv("data/house party vote share by district 1948-2020.csv") %>% 
  filter(raceYear == 2018) %>%
  rowwise() %>%
  mutate(District = ifelse(district_num == 0, paste0(state_abb, "-01"), CD)) %>%
  ungroup()

df2018 = votes2018 %>%
  inner_join(experts2018, "District") %>%
  filter(State != "Alaska" & State != "Hawaii")
  
percentile = function(column) {100 * ecdf(column)(column)}
df2018$cpr_num = percentile(df2018$cpr_num)
df2018$crystal_ball_num = percentile(df2018$crystal_ball_num)
df2018$inside_elections_num = percentile(df2018$inside_elections_num)
df2018$avg = percentile(df2018$avg)
df2018$RepVotesMajorPercent = percentile(df2018$RepVotesMajorPercent)
df2018$Difference = df2018$RepVotesMajorPercent - df2018$avg
```

```{r congressmap, include=FALSE}
# load geographic data
get_congress_map = function() {
  tmp_file = tempfile()
  tmp_dir = tempdir()
  download.file("https://cdmaps.polisci.ucla.edu/shp/districts114.zip", tmp_file)
  unzip(zipfile=tmp_file, exdir=tmp_dir)
  fpath = paste(tmp_dir, "districtShapes/districts114.shp", sep="/")
  st_read(fpath)
}

districts = get_congress_map()
```

```{r maps, include=FALSE}
plot_map = function(colname, title, limits) {
  df2018["Percentile"] = df2018[colname]
  districts$DISTRICT = as.numeric(districts$DISTRICT)
  districts_simp <- ms_simplify(inner_join(districts, df2018, c("STATENAME" = "State", "DISTRICT" = "district_num")), keep = 0.01)
  plt = ggplot() +
    geom_sf(data=districts_simp, aes(fill=Percentile),
            inherit.aes=FALSE, alpha=0.9) +
    scale_fill_gradient(low="blue", high="red", limits=limits) +
    coord_sf(xlim=c(-124.8, -66.9), ylim=c(24.5, 49.4), expand=FALSE) +
    theme_void() +
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank()) + 
    ggtitle(title)
  return(plt)
}
```

```{r correlations, include=FALSE}
cor(df2018$RepVotesMajorPercent, df2018$avg)
cor(df2018$RepVotesMajorPercent, df2018$crystal_ball_num)
cor(df2018$RepVotesMajorPercent, df2018$cpr_num)
cor(df2018$RepVotesMajorPercent, df2018$inside_elections_num)
```

```{r maps1, include=TRUE, fig.width=10, fig.height=5}
p1 = plot_map("RepVotesMajorPercent", "Republican Two-Party Vote Share by District", c(0, 100))
p2 = plot_map("avg", "Average Expert Prediction by District", c(0, 100))
grid.arrange(p1, p2, ncol=2)
```

The two maps look pretty similar. Indeed, the correlation between the percentile of the actual Republican vote share and the percentile of the average forecaster rating is roughly 0.9, indicating a very strong relationship between the two. The districts that are purple in the left map - meaning a vote share around 50% - tend to also be purple in the right map - meaning an average rating close to a toss-up. The same trend seems to hold with districts that are very red (meaning very Republican-favored) in both maps. However, the main differences between the two maps arise in the districts that are very blue in the left map - meaning very Democrat-favored. These states tend to be purpler on the right map, which perhaps means that experts were not as good at predicting Democrat landslides as they were at predicting Republican landslides and toss-ups. 

This pattern is verified by plotting the difference between the percentile of the actual Republican vote share and the percentile of the average expert rating, on the map below.

```{r maps2, include=TRUE, fig.width=10, fig.height=5}
plot_map("Difference", "Difference Between Actual Republican Vote Share and Average Expert Prediction", c(-50, 50))
```

There are pretty clearly more blueish districts (meaning Democrats did better than expected) than reddish districts (meaning Republicans did better than expected). We know that the Democrats did very well in the 2018 midterms, so perhaps all this indicates that they did better than they were expected to at the time.

This analysis does not change very much if we were to look at any individual forecaster as opposed to the average of the forecasters. Inside Elections had predictions that were slightly less accurate (a correlation of 0.89 with actual vote outcomes), and the other two forecasters had predictions that were slightly more accurate (correlations of roughly 0.91 with actual vote outcomes). But the trend of under-predicting Democrat landslides remained. 

## Model Update

This week, I update my forecasting model to include the effects of incumbency, arguably the most important fundamental in an election. We saw in [previous weeks the effect of the party of the sitting president](https://jrdelgado2018.github.io/GOV1347/blogs/blog2/Blog-Two.html), but this week I will consider the incumbency advantage that a member of Congress might have.

So, I manipulate the data to include flags for whether each candidate is an incumbent or not. In line with this week's videocast's discussion of the effect of mean-reversion, I also include a flag for whether it is the incumbent's first term in office. The logic here is that a politician who was just elected to their term was likely elected in a year in which their party did better than average, so mean reversion will see them do worse the next year on average. 

As before, I aggregate the economic predictor and these new fundamental variables on the state level, and include national polling through the generic ballot. Also as before, I allow the presence of a Democrat president in office interact with the coefficient for unemployment and generic ballot polling, since we've seen [in previous weeks that voters react differently depending on the party of the president](https://jrdelgado2018.github.io/GOV1347/blogs/blog3/Blog-Three.html).

```{r data, include=FALSE}
# Read in the voting data, filter for elections where there was a Democrat and a Republican and third parties were < 10% of the vote, 
# and create columns for whether either candidate is the incumbent
votes = read_csv("data/house party vote share by district 1948-2020.csv") %>% 
  filter(ThirdVotesTotalPercent < 10 & !is.na(RepCandidate) & !is.na(DemCandidate)) %>%
  mutate(DemIncumbent = DemStatus == "Incumbent", 
         RepIncumbent = RepStatus == "Incumbent")

# Create a dataframe with the relevant variables from the prior election (will need this later)
df_prev1 = votes %>%
  rename(DemIncPrev = DemIncumbent, RepIncPrev = RepIncumbent) %>%
  mutate(raceYear = raceYear + 2) %>%
  select(raceYear, CD, DemIncPrev, RepIncPrev)

# Join with the data from the previous election, and group by state instead of district
votes = votes %>% 
  inner_join(df_prev1, c("raceYear", "CD")) %>%
  mutate(DemIncFirstTerm = DemIncumbent & !DemIncPrev, 
         RepIncFirstTerm = RepIncumbent & !RepIncPrev) %>%
  group_by(raceYear, State) %>% 
  summarize(DemPct = 100 * sum(DemVotes) / (sum(DemVotes) + sum(RepVotes)), 
            DemIncumbent = mean(DemIncumbent), 
            RepIncumbent = mean(RepIncumbent),
            DemIncFirstTerm = mean(DemIncFirstTerm), 
            RepIncFirstTerm = mean(RepIncFirstTerm),
            .groups = 'drop') %>%
  mutate(DemPresident = raceYear %in% c(1978, 1980, 1994, 1996, 1998, 2000, 2010, 2012, 2014, 2016))

# Create a dataframe with the relevant variables from the prior election (will need this later)
df_prev2 = votes %>%
  rename(PctPrev = DemPct) %>%
  mutate(raceYear = raceYear + 2) %>%
  select(raceYear, State, PctPrev)

# Read in the state unemployment data, filtering for the Q3 and Q4 numbers
unemp_Q4 = read_csv("data/unemployment_state_monthly.csv") %>% 
  filter(Month == 10) %>%
  rename(UnempQ4 = Unemployed_prct) %>%
  select(Year, `State and area`, UnempQ4)
unemp_Q3 = read_csv("data/unemployment_state_monthly.csv") %>% 
  filter(Month == 7) %>%
  rename(UnempQ3 = Unemployed_prct) %>%
  select(Year, `State and area`, UnempQ3)

# Join the unemployment data together
unemp = inner_join(unemp_Q4, unemp_Q3, c("Year", "State and area"))

# Get the poll data, filter for polls within 2 months of the election, and take simple average of results
polls = read_csv("data/polls_df.csv") %>% 
  filter(emonth %in% c(10, 11)) %>%
  mutate(isDemocrat = party == "D") %>%
  group_by(year) %>%
  summarize(D = sum(support * isDemocrat) / sum(isDemocrat), R = sum(support * (1 - isDemocrat)) / sum(1 - isDemocrat)) %>%
  mutate(DemPolls = D / (D + R))

# Join the data all together, calculating a couple of extra columns
df = votes %>% 
  inner_join(df_prev2, c("raceYear", "State")) %>%
  inner_join(unemp, c("raceYear" = "Year", "State" = "State and area")) %>%
  inner_join(polls, c("raceYear" = "year")) %>%
  mutate(UnempPctChange = 100 * (UnempQ4 - UnempQ3) / UnempQ3,
         DemTimesUnemp = DemPresident * UnempPctChange, 
         DemTimesPolls = DemPresident * DemPolls)
```

Incorporating these new variables into the model raises the R-squared to 0.71, an increase of roughly 0.1 from last week! Moreover, the standard error of the residual and bootstrapped estimate of the root-mean square error jump down by almost one point to roughly 5.7, meaning that our estimates got much more precise. 

Interestingly, the variables for whether the incumbent candidate was in their first term did not prove to be significant. Perhaps this is because mean reversion is literally baked into the fitted regression equation (in this context, it's known as ["regression to the mean"](https://en.wikipedia.org/wiki/Regression_toward_the_mean#Definition_for_simple_linear_regression_of_data_points)), so we do not actually have to include variables that are meant to capture mean reversion.

Below is the usual scatterplot showing predicted vote share versus actual vote share, as well as the histogram of residuals. We see that the data points tend to be much closer to the 45-degree line than they were last week, and that the residuals are once again approximately Normal (but with a smaller standard deviation than last week).

```{r model2, include=FALSE}
# Model incorporating the previous vote share, the unemployment level, and generic ballot polling
model1 = lm(DemPct ~ PctPrev + DemPolls + DemTimesPolls + UnempPctChange + DemTimesUnemp + DemIncumbent + RepIncumbent + DemIncFirstTerm + RepIncFirstTerm, df)
summary(model1)

model2 = lm(DemPct ~ PctPrev + DemPolls + DemTimesPolls + UnempPctChange + DemTimesUnemp + DemIncumbent + RepIncumbent, df)
df$Predicted2 = model2$fitted.values
summary(model2)

# Bootstrapped estimate of the MSE using cross-validation
mses = replicate(10000, {
  in_sample_inds = sample.int(n=nrow(df), size=floor(0.75*nrow(df)), replace=FALSE)
  in_sample = df[in_sample_inds, ]
  out_sample = df[-in_sample_inds, ]
  mod = lm(DemPct ~ PctPrev + DemPolls + DemTimesPolls + UnempPctChange + DemTimesUnemp + DemIncumbent + RepIncumbent, in_sample)
  preds = predict(mod, out_sample)
  mean((preds - out_sample$DemPct) ** 2)
})
mse = mean(mses)
rmse = sqrt(mse)
```

```{r model3, include=TRUE, fig.width=10, fig.height=5}
# Scatterplot of predicted vs. actual vote share
p3 = df %>% ggplot(aes(Predicted2, DemPct)) +
  geom_point() + 
  geom_abline(slope=1, intercept=0, lty=2) +
  xlab("Predicted Vote Share Achieved by Democrats") +
  ylab("Actual Vote Share Achieved by Democrats") +
  ggtitle("(vote share) = 25.9 + \n(10.6 if Dem. is incumbent) + \n(-10.9 if Rep. is incumbent) + \n0.29 (previous vote share) + \n(0.10 if Rep. president else -0.10) (unemp. % change) + \n(19.7 if Rep. president else 15.3) (generic ballot)")

# Histogram of the residual with overlayed Normal density curve
p4 = df %>% ggplot(aes(Predicted2 - DemPct)) + 
  geom_histogram(aes(y = ..density..), binwidth=4) + 
  xlab("Residual") + 
  ylab("Density") + 
  stat_function(fun = function(x) dnorm(x, 0, summary(model2)$sigma)) + 
  ggtitle("Histogram of Residuals")

grid.arrange(p3, p4, ncol=2)
```

Below is a map summarizing the current model's vote share predictions for each state in the 2022 midterms. I again use the most current available unemployment and generic ballot data. South Dakota again does not have a prediction because there is no Democrat running in the state. A color closer to blue corresponds to a larger vote share for Democrats, and a color closer to white corresponds to a smaller vote share for Democrats. Below is also a table that gives a 95% prediction interval for each state.

```{r map, include=TRUE, fig.width=10, fig.height=5, message=FALSE}
# Get the polls for today
polls_today = read_csv("data/538_generic_ballot_averages_2018-2022.csv") %>% 
  filter(month == 9 & cycle == 2022) %>%
  mutate(isDemocrat = candidate == "Democrats") %>%
  group_by(cycle) %>%
  summarize(D = sum(pct_estimate * isDemocrat) / sum(isDemocrat), R = sum(pct_estimate * (1 - isDemocrat)) / sum(1 - isDemocrat)) %>%
  mutate(DemPolls = D / (D + R))

# Read in the state unemployment data, filtering for the Q3 and Q4 numbers
unemp_Q4_today = read_csv("data/unemployment_state_monthly.csv") %>% 
  filter(Month == 5 & Year == 2022) %>%
  rename(UnempQ4 = Unemployed_prct) %>%
  select(Year, `State and area`, UnempQ4)
unemp_Q3_today = read_csv("data/unemployment_state_monthly.csv") %>% 
  filter(Month == 2 & Year == 2022) %>%
  rename(UnempQ3 = Unemployed_prct) %>%
  select(Year, `State and area`, UnempQ3)
unemp_today = inner_join(unemp_Q4_today, unemp_Q3_today, c("Year", "State and area"))

# Get the incumbency data specifically for today
inc = read_csv("data/incumb_dist_1948-2022.csv") %>%
  filter(year == 2020) %>%
  mutate(RepIncumbent = winner_candidate == "RepCandidate",
         DemIncumbent = winner_candidate == "DemCandidate") %>%
  group_by(state) %>%
  summarize(DemIncumbent = mean(DemIncumbent), 
            RepIncumbent = mean(RepIncumbent), 
            .groups = "drop")

# Get the data specifically for today
today = unemp_today %>%
  inner_join(filter(df_prev2, raceYear == 2022), c("State and area" = "State")) %>%
  rename(state = `State and area`) %>%
  inner_join(inc, "state") %>%
  inner_join(polls_today, c("Year" = "cycle")) %>%
  mutate(UnempPctChange = 100 * (UnempQ4 - UnempQ3) / UnempQ3,
         DemTimesUnemp = UnempPctChange, 
         DemTimesPolls = DemPolls)

# Get the predictions given by this model
today$Predicted = data.frame(predict(model2, today, interval="prediction"))$fit
today$LowerBound = data.frame(predict(model2, today, interval="prediction"))$lwr
today$UpperBound = data.frame(predict(model2, today, interval="prediction"))$upr

# Plot a US map with the predictions
states = us_map()
plot_usmap(data=today, regions="states", values="Predicted") +
  theme_void() + 
  scale_fill_gradient(low="white", high="blue", name="% of Votes", limits=range(30, 70)) + 
  ggtitle("Predicted Vote Share Achieved by Democrats, 2022 Election")

# Print the predictions and the prediction interval
print(data.frame(select(today, state, LowerBound, Predicted, UpperBound)), digits = 3)
```



